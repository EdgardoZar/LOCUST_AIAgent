pipeline {
    agent any
    
    parameters {
        // Test execution parameters will be selected during the build
        string(name: 'TARGET_HOST', defaultValue: 'https://rickandmortyapi.com/api', description: 'Target host URL for testing')
        string(name: 'API_TOKEN', defaultValue: '', description: 'API token for authentication (optional)')
        choice(name: 'ENVIRONMENT', choices: ['dev', 'staging', 'prod'], description: 'Target environment')
        
        // Test configuration
        string(name: 'USERS', defaultValue: '10', description: 'Number of concurrent users')
        
        // Test configuration
        string(name: 'SPAWN_RATE', defaultValue: '2', description: 'User spawn rate per second')
        string(name: 'RUN_TIME', defaultValue: '5m', description: 'Test duration (e.g., 5m, 10m, 1h)')
        string(name: 'MIN_WAIT', defaultValue: '1000', description: 'Minimum wait time between requests (ms)')
        string(name: 'MAX_WAIT', defaultValue: '5000', description: 'Maximum wait time between requests (ms)')
        
        // Advanced options
        booleanParam(name: 'USE_LLM_ANALYSIS', defaultValue: false, description: 'Enable LLM-powered analysis')
        booleanParam(name: 'GENERATE_HTML_REPORT', defaultValue: true, description: 'Generate HTML test report')
        booleanParam(name: 'GENERATE_CSV_REPORT', defaultValue: true, description: 'Generate CSV test report')
        string(name: 'LOG_LEVEL', defaultValue: 'INFO', description: 'Logging level (DEBUG, INFO, WARNING, ERROR)')
        
        // Performance thresholds
        string(name: 'MAX_AVG_RESPONSE_TIME', defaultValue: '2000', description: 'Maximum average response time (ms)')
        string(name: 'MIN_SUCCESS_RATE', defaultValue: '95', description: 'Minimum success rate percentage')
        string(name: 'MIN_REQUESTS_PER_SEC', defaultValue: '10', description: 'Minimum requests per second')
        
        // Test metadata
        string(name: 'TEST_DESCRIPTION', defaultValue: '', description: 'Description of this test run (optional)')
        string(name: 'TEST_TAGS', defaultValue: '', description: 'Tags for this test run (optional)')
    }
    
    environment {
        // Python and workspace configuration
        PYTHON_VERSION = "${env.PYTHON_VERSION ?: '3.9'}"
        WORKSPACE_DIR = "${WORKSPACE}\\test_workspace"
        REPORTS_DIR = "${WORKSPACE}\\test_reports"
        SCRIPTS_DIR = "${WORKSPACE}\\generated_scripts"
        TIMESTAMP = "${new Date().format('yyyyMMdd_HHmmss')}"
        
        // Build information
        BUILD_INFO = "Build #${env.BUILD_NUMBER} - ${env.BUILD_ID}"
        JOB_NAME = "${env.JOB_NAME}"
        BUILD_URL = "${env.BUILD_URL}"
        
        // Test execution settings
        TEST_RUN_ID = "${env.BUILD_NUMBER}_${TIMESTAMP}"
        TEST_ENVIRONMENT = "${params.ENVIRONMENT}"
        
        // Performance thresholds (convert to integers)
        MAX_RESPONSE_TIME_MS = "${params.MAX_AVG_RESPONSE_TIME}"
        MIN_SUCCESS_RATE_PCT = "${params.MIN_SUCCESS_RATE}"
        MIN_REQUESTS_PER_SEC = "${params.MIN_REQUESTS_PER_SEC}"
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout([$class: 'GitSCM', 
                    branches: [[name: '*/main']], 
                    doGenerateSubmoduleConfigurations: false, 
                    extensions: [], 
                    submoduleCfg: [], 
                    userRemoteConfigs: [[
                        credentialsId: 'github-credentials',
                        url: 'https://github.com/EdgardoZar/LOCUST_AIAgent.git'
                    ]]
                ])
                script {
                    echo "=== Test Execution Pipeline ==="
                    echo "Build Number: ${env.BUILD_NUMBER}"
                    echo "Job Name: ${env.JOB_NAME}"
                    echo "Workspace: ${WORKSPACE}"
                    echo "Branch: ${env.BRANCH_NAME}"
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    echo "Environment: ${TEST_ENVIRONMENT}"
                    echo "Target Host: ${params.TARGET_HOST}"
                }
            }
        }
        
        stage('Setup Environment') {
            steps {
                script {
                    echo "Setting up test environment..."
                    echo "Python Version: ${PYTHON_VERSION}"
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    echo "Performance Thresholds:"
                    echo "  Max Response Time: ${MAX_RESPONSE_TIME_MS}ms"
                    echo "  Min Success Rate: ${MIN_SUCCESS_RATE_PCT}%"
                    echo "  Min Requests/sec: ${MIN_REQUESTS_PER_SEC}"
                    
                    // Create directories
                    bat "if not exist \"${WORKSPACE_DIR}\" mkdir \"${WORKSPACE_DIR}\""
                    bat "if not exist \"${REPORTS_DIR}\" mkdir \"${REPORTS_DIR}\""
                    bat "if not exist \"${SCRIPTS_DIR}\" mkdir \"${SCRIPTS_DIR}\""
                    
                    // Setup Python virtual environment
                    bat """
                        if not exist venv (
                            python -m venv venv
                        )
                        call venv\\Scripts\\activate.bat
                        pip install --upgrade pip
                        pip install -r requirements.txt
                        pip install -e .
                    """
                }
            }
        }
        
        stage('Select Script') {
            steps {
                script {
                    echo "üîç Finding available scripts..."
                    
                    // Use a more robust method to get script files
                    def scriptFiles = ""
                    try {
                        def dirOutput = bat(script: "dir \"${SCRIPTS_DIR}\\*.py\" /b", returnStdout: true).trim()
                        if (dirOutput && !dirOutput.contains("File Not Found") && !dirOutput.isEmpty()) {
                            scriptFiles = dirOutput
                        }
                    } catch (Exception e) {
                        error "Could not list scripts: ${e.getMessage()}"
                    }
                    
                    if (!scriptFiles) {
                        error "‚ùå No scripts found in ${SCRIPTS_DIR}. Please run the Script Generation Pipeline first."
                    }
                    
                    def scriptList = scriptFiles.tokenize('\n').collect { it.trim() }
                    echo "‚úÖ Found scripts: ${scriptList}"

                    // Now, ask the user to choose using an input step
                    def userInput = input(
                        id: 'scriptSelection',
                        message: 'Please select the test script to run',
                        parameters: [
                            choice(
                                name: 'SELECTED_SCRIPT',
                                choices: scriptList,
                                description: 'Choose a script from the list of available scripts.'
                            )
                        ]
                    )
                    
                    echo "üéØ User selected script: ${userInput}"
                    env.SELECTED_SCRIPT = userInput
                }
            }
        }
        
        stage('Validate Script') {
            steps {
                script {
                    echo "Validating selected script: ${env.SELECTED_SCRIPT}"
                    
                    // Check if a valid script was selected
                    if (env.SELECTED_SCRIPT == null || env.SELECTED_SCRIPT.isEmpty()) {
                        error "No script was selected. Aborting."
                    }
                    
                    // Check if script exists
                    def scriptExists = fileExists "${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}"
                    if (!scriptExists) {
                        error "Script '${env.SELECTED_SCRIPT}' not found. It might have been deleted from the repository. Please try running the build again."
                    }
                    
                    // Store script info in environment
                    env.SCRIPT_NAME = env.SELECTED_SCRIPT.replace('.py', '')
                    env.SCRIPT_PATH = "${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}"
                    
                    echo "Script validation successful"
                    echo "Script Name: ${env.SCRIPT_NAME}"
                    echo "Script Path: ${env.SCRIPT_PATH}"
                }
            }
        }
        
        stage('Run Test') {
            steps {
                script {
                    echo "Running test with script: ${env.SELECTED_SCRIPT}"
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    echo "Environment: ${TEST_ENVIRONMENT}"
                    echo "Target Host: ${params.TARGET_HOST}"
                    echo "Users: ${params.USERS}"
                    echo "Run Time: ${params.RUN_TIME}"
                    
                    // Create test configuration
                    def testConfig = [
                        scenario_name: env.SCRIPT_NAME,
                        host: params.TARGET_HOST,
                        users: params.USERS,
                        spawn_rate: params.SPAWN_RATE,
                        run_time: params.RUN_TIME,
                        min_wait: params.MIN_WAIT,
                        max_wait: params.MAX_WAIT,
                        assertions: [
                            [type: "status_code", value: 200]
                        ],
                        extract_variables: [:],
                        headers: [
                            "Content-Type": "application/json",
                            "User-Agent": "Jenkins-Locust-AI-Agent/1.0"
                        ],
                        params: [:],
                        body: [:],
                        output_dir: REPORTS_DIR,
                        generate_csv: params.GENERATE_CSV_REPORT,
                        generate_html: params.GENERATE_HTML_REPORT,
                        log_level: params.LOG_LEVEL
                    ]
                    
                    writeFile file: "${WORKSPACE_DIR}\\test_config.json", text: groovy.json.JsonOutput.toJson(testConfig)
                    
                    // Script is already available in generated_scripts folder on main branch
                    echo "Using script: ${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}"
                    
                    // Write the Python script to a file
                    def executeTestScript = """
import sys
import os

# Add the workspace to Python path
workspace = r"${WORKSPACE}"
sys.path.insert(0, workspace)
sys.path.insert(0, os.path.join(workspace, "core"))
sys.path.insert(0, os.path.join(workspace, "analysis"))
sys.path.insert(0, os.path.join(workspace, "utils"))

print(f"DEBUG: Python path: {sys.path}")
print(f"DEBUG: Current working directory: {os.getcwd()}")
print(f"DEBUG: Workspace: {workspace}")

try:
    from core.test_agent import LocustTestAgent, TestConfig
    print("DEBUG: Successfully imported LocustTestAgent and TestConfig")
except ImportError as e:
    print(f"DEBUG: Import error: {e}")
    print("DEBUG: Available modules:", [m for m in sys.modules.keys() if 'locust' in m.lower()])
    sys.exit(1)

import json

# Load test config
with open(r"${WORKSPACE_DIR}\\test_config.json", "r") as f:
    test_config_data = json.load(f)

# Create test config
test_config = TestConfig(**test_config_data)

# Run test
agent = LocustTestAgent(workspace_dir=r"${WORKSPACE_DIR}")
script_path = r"${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}"
result = agent.execute_test(script_path, test_config)

# Save results
workflow_result = {
    "workflow_success": result.success,
    "scenario_name": result.scenario_name,
    "script_path": result.script_path,
    "html_report_path": result.html_report_path,
    "csv_report_path": result.csv_report_path,
    "test_metadata": {
        "test_run_id": "${TEST_RUN_ID}",
        "environment": "${TEST_ENVIRONMENT}",
        "build_number": "${env.BUILD_NUMBER}",
        "job_name": "${env.JOB_NAME}",
        "description": "${params.TEST_DESCRIPTION}",
        "tags": "${params.TEST_TAGS}"
    },
    "test_result": {
        "execution_time": result.execution_time,
        "total_requests": result.total_requests,
        "failed_requests": result.failed_requests,
        "avg_response_time": result.avg_response_time,
        "requests_per_sec": result.requests_per_sec
    }
}

with open(r"${WORKSPACE_DIR}\\test_results.json", "w") as f:
    json.dump(workflow_result, f, indent=2)

print(f"Test completed: Success={result.success}")
"""
                    writeFile file: "${WORKSPACE_DIR}\\execute_test.py", text: executeTestScript
                    
                    bat """
                        call venv\\Scripts\\activate.bat
                        cd ${WORKSPACE}
                        set PYTHONPATH=%PYTHONPATH%;${WORKSPACE};${WORKSPACE}\\core;${WORKSPACE}\\analysis;${WORKSPACE}\\utils
                        echo DEBUG: PYTHONPATH=%PYTHONPATH%
                        echo DEBUG: About to run: python ${WORKSPACE_DIR}\\execute_test.py
                        python ${WORKSPACE_DIR}\\execute_test.py
                    """
                    
                    // Store test results in environment
                    def resultsText = readFile file: "${WORKSPACE_DIR}\\test_results.json"
                    
                    // Parse results using Python to avoid serialization issues
                    def processResultsScript = """
import sys
import json
import os

try:
    # Load test results
    with open(r"${WORKSPACE_DIR}\\test_results.json", "r") as f:
        results = json.load(f)
    
    # Extract and validate data
    workflow_success = results.get("workflow_success", False)
    scenario_name = results.get("scenario_name", "Unknown")
    script_path = results.get("script_path", "Unknown")
    
    test_result = results.get("test_result", {})
    total_requests = test_result.get("total_requests", 0)
    failed_requests = test_result.get("failed_requests", 0)
    avg_response_time = test_result.get("avg_response_time", 0)
    requests_per_sec = test_result.get("requests_per_sec", 0)
    execution_time = test_result.get("execution_time", 0)
    html_report_path = test_result.get("html_report_path", "")
    csv_report_path = test_result.get("csv_report_path", "")
    
    llm_analysis = results.get("llm_analysis", {})
    performance_grade = llm_analysis.get("performance_grade", "N/A")
    summary = llm_analysis.get("summary", "N/A")
    key_insights = llm_analysis.get("key_insights", [])
    recommendations = llm_analysis.get("recommendations", [])
    
    # Create metadata for Jenkins
    metadata = {
        "workflow_success": workflow_success,
        "scenario_name": scenario_name,
        "script_path": script_path,
        "test_result": {
            "execution_time": execution_time,
            "total_requests": total_requests,
            "failed_requests": failed_requests,
            "avg_response_time": avg_response_time,
            "requests_per_sec": requests_per_sec
        },
        "llm_analysis": {
            "performance_grade": performance_grade,
            "summary": summary,
            "key_insights": key_insights,
            "recommendations": recommendations
        },
        "success": True
    }
    
    with open(r"${WORKSPACE_DIR}\\test_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    
    print(f"Test results processed successfully")
    print(f"Success: {workflow_success}")
    print(f"Total Requests: {total_requests}")
    print(f"Failed Requests: {failed_requests}")
    print(f"Avg Response Time: {avg_response_time}ms")
    print(f"Requests/sec: {requests_per_sec}")
    
except Exception as e:
    print(f"Error processing test results: {str(e)}")
    metadata = {
        "success": False,
        "error": str(e)
    }
    with open(r"${WORKSPACE_DIR}\\test_metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
    sys.exit(1)
"""
                    writeFile file: "${WORKSPACE_DIR}\\process_results.py", text: processResultsScript
                    
                    // Parse results using Python to avoid serialization issues
                    bat """
                        call venv\\Scripts\\activate.bat
                        cd ${WORKSPACE}
                        set PYTHONPATH=%PYTHONPATH%;${WORKSPACE}
                        python ${WORKSPACE_DIR}\\process_results.py
                    """
                    
                    // Read test metadata
                    def metadataText = readFile file: "${WORKSPACE_DIR}\\test_metadata.json"
                    def metadata = new groovy.json.JsonSlurper().parseText(metadataText)
                    
                    if (metadata.success) {
                        env.TEST_SUCCESS = metadata.workflow_success.toString()
                        env.TOTAL_REQUESTS = metadata.test_result.total_requests.toString()
                        env.FAILED_REQUESTS = metadata.test_result.failed_requests.toString()
                        env.AVG_RESPONSE_TIME = metadata.test_result.avg_response_time.toString()
                        env.REQUESTS_PER_SEC = metadata.test_result.requests_per_sec.toString()
                        env.EXECUTION_TIME = metadata.test_result.execution_time.toString()
                        
                        echo "Test execution completed"
                        echo "Success: ${env.TEST_SUCCESS}"
                        echo "Total Requests: ${env.TOTAL_REQUESTS}"
                        echo "Failed Requests: ${env.FAILED_REQUESTS}"
                        echo "Avg Response Time: ${env.AVG_RESPONSE_TIME}ms"
                        echo "Requests/sec: ${env.REQUESTS_PER_SEC}"
                    } else {
                        error "Failed to process test results: ${metadata.error}"
                    }
                }
            }
        }
        
        stage('LLM Analysis') {
            when {
                expression { params.USE_LLM_ANALYSIS }
            }
            steps {
                script {
                    echo "Running LLM analysis..."
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    
                    withCredentials([string(credentialsId: 'openai-api-key', variable: 'OPENAI_API_KEY')]) {
                        // Write the Python script to a file
                        def llmAnalysisScript = """
import sys
import os
import json

# Add the workspace to Python path
workspace = r"${WORKSPACE}"
sys.path.insert(0, workspace)
sys.path.insert(0, os.path.join(workspace, "analysis"))

print(f"DEBUG: LLM Analysis Python path: {sys.path}")

try:
    from analysis.llm_analyzer import LLMAnalyzer
    print("DEBUG: Successfully imported LLMAnalyzer")
except ImportError as e:
    print(f"DEBUG: LLM Import error: {e}")
    sys.exit(1)

# Load test results
with open(r"${WORKSPACE_DIR}\\test_results.json", "r") as f:
    results = json.load(f)

# Run LLM analysis
llm_analyzer = LLMAnalyzer(api_key="${OPENAI_API_KEY}")
llm_analysis = llm_analyzer.analyze_test_results(
    results["test_result"],
    results.get("html_report_path")
)

# Add LLM analysis to results
results["llm_analysis"] = llm_analysis

# Save updated results
with open(r"${WORKSPACE_DIR}\\test_results_with_llm.json", "w") as f:
    json.dump(results, f, indent=2)

print("LLM analysis completed")
"""
                        writeFile file: "${WORKSPACE_DIR}\\llm_analysis.py", text: llmAnalysisScript
                        
                        bat """
                            call venv\\Scripts\\activate.bat
                            cd ${WORKSPACE}
                            set PYTHONPATH=%PYTHONPATH%;${WORKSPACE}
                            set OPENAI_API_KEY=${OPENAI_API_KEY}
                            python ${WORKSPACE_DIR}\\llm_analysis.py
                        """
                    }
                    
                    echo "LLM analysis completed successfully"
                }
            }
        }
        
        stage('Archive Results') {
            steps {
                script {
                    echo "Archiving test results..."
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    
                    // Archive HTML report if it exists
                    archiveArtifacts(
                        artifacts: 'test_reports/**/*.html', 
                        allowEmptyArchive: true, 
                        fingerprint: true
                    )
                    
                    // Archive CSV report if it exists
                    archiveArtifacts(
                        artifacts: 'test_reports/**/*.csv', 
                        allowEmptyArchive: true, 
                        fingerprint: true
                    )

                    // Archive JSON results
                    archiveArtifacts(
                        artifacts: 'test_workspace/test_results.json', 
                        allowEmptyArchive: true, 
                        fingerprint: true
                    )
                }
            }
        }
        
        stage('Analyze Results') {
            steps {
                script {
                    echo "Analyzing test results..."
                    echo "Performance Thresholds:"
                    echo "  Max Response Time: ${MAX_RESPONSE_TIME_MS}ms"
                    echo "  Min Success Rate: ${MIN_SUCCESS_RATE_PCT}%"
                    echo "  Min Requests/sec: ${MIN_REQUESTS_PER_SEC}"
                    
                    // Load and analyze test results using metadata from previous stage
                    def metadataText = readFile file: "${WORKSPACE_DIR}\\test_metadata.json"
                    def metadata = new groovy.json.JsonSlurper().parseText(metadataText)
                    
                    if (!metadata.success) {
                        error "Cannot analyze results: ${metadata.error}"
                    }
                    
                    def results = metadata
                    
                    echo "Test Results Analysis:"
                    echo "  Success: ${results.workflow_success}"
                    echo "  Scenario: ${results.scenario_name}"
                    echo "  Script: ${env.SELECTED_SCRIPT}"
                    echo "  Test Run ID: ${TEST_RUN_ID}"
                    
                    if (results.test_result) {
                        def testResult = results.test_result
                        echo "  Execution Time: ${testResult.execution_time}s"
                        echo "  Total Requests: ${testResult.total_requests}"
                        echo "  Failed Requests: ${testResult.failed_requests}"
                        echo "  Avg Response Time: ${testResult.avg_response_time}ms"
                        echo "  Requests/sec: ${testResult.requests_per_sec}"
                        
                        // Calculate success rate
                        def successRate = testResult.total_requests.toInteger() > 0 ? 
                            ((testResult.total_requests.toInteger() - testResult.failed_requests.toInteger()) / testResult.total_requests.toInteger() * 100) : 0
                        echo "  Success Rate: ${successRate}%"
                        
                        // Store calculated values in environment
                        env.SUCCESS_RATE = successRate
                        
                        // Performance threshold validation
                        def performanceIssues = []
                        
                        if (testResult.avg_response_time.toInteger() > MAX_RESPONSE_TIME_MS.toInteger()) {
                            performanceIssues.add("Average response time (${testResult.avg_response_time}ms) exceeds threshold (${MAX_RESPONSE_TIME_MS}ms)")
                        }
                        
                        if (successRate < MIN_SUCCESS_RATE_PCT.toInteger()) {
                            performanceIssues.add("Success rate (${successRate}%) below threshold (${MIN_SUCCESS_RATE_PCT}%)")
                        }
                        
                        if (testResult.requests_per_sec.toDouble() < MIN_REQUESTS_PER_SEC.toInteger()) {
                            performanceIssues.add("Requests per second (${testResult.requests_per_sec}) below threshold (${MIN_REQUESTS_PER_SEC})")
                        }
                        
                        if (performanceIssues.size() > 0) {
                            echo "Performance Issues Detected:"
                            performanceIssues.each { issue ->
                                echo "  - ${issue}"
                            }
                            currentBuild.result = 'UNSTABLE'
                            env.PERFORMANCE_ISSUES = performanceIssues.join('; ')
                        } else {
                            echo "All performance thresholds met!"
                            env.PERFORMANCE_ISSUES = "None"
                        }
                    }
                    
                    // LLM analysis summary
                    if (params.USE_LLM_ANALYSIS && results.llm_analysis) {
                        def llmAnalysis = results.llm_analysis
                        echo "LLM Analysis:"
                        echo "  Performance Grade: ${llmAnalysis.performance_grade}"
                        echo "  Summary: ${llmAnalysis.summary}"
                        
                        env.LLM_GRADE = llmAnalysis.performance_grade
                        env.LLM_SUMMARY = llmAnalysis.summary
                        
                        if (llmAnalysis.key_insights) {
                            echo "  Key Insights:"
                            llmAnalysis.key_insights.each { insight ->
                                echo "    ‚Ä¢ ${insight}"
                            }
                        }
                        
                        if (llmAnalysis.recommendations) {
                            echo "  Recommendations:"
                            llmAnalysis.recommendations.each { rec ->
                                echo "    ‚Ä¢ ${rec}"
                            }
                        }
                    }
                }
            }
        }
        
        stage('Publish HTML Report') {
            when {
                expression { params.GENERATE_HTML_REPORT }
            }
            steps {
                script {
                    echo "Publishing HTML report..."
                    
                    // Check if HTML report exists
                    def htmlReportExists = fileExists "test_reports\\*.html"
                    if (htmlReportExists) {
                        publishHTML([
                            allowMissing: false,
                            alwaysLinkToLastBuild: true,
                            keepAll: true,
                            reportDir: 'test_reports',
                            reportFiles: '*.html',
                            reportName: 'Locust Test Report',
                            reportTitles: 'Performance Test Results'
                        ])
                        echo "HTML report published successfully"
                    } else {
                        echo "No HTML report found to publish"
                    }
                }
            }
        }
    }
    
    post {
        always {
            script {
                echo "=== Test Execution Pipeline Summary ==="
                echo "Build Number: ${env.BUILD_NUMBER}"
                echo "Job Name: ${env.JOB_NAME}"
                echo "Result: ${currentBuild.result}"
                echo "Build URL: ${env.BUILD_URL}"
                echo "Test Run ID: ${TEST_RUN_ID}"
                echo "Script Used: ${env.SELECTED_SCRIPT}"
                echo "Environment: ${TEST_ENVIRONMENT}"
                echo "Target Host: ${params.TARGET_HOST}"
                echo "Test Success: ${env.TEST_SUCCESS}"
                echo "Total Requests: ${env.TOTAL_REQUESTS}"
                echo "Success Rate: ${env.SUCCESS_RATE}%"
                echo "Avg Response Time: ${env.AVG_RESPONSE_TIME}ms"
                echo "Performance Issues: ${env.PERFORMANCE_ISSUES}"
                
                if (params.USE_LLM_ANALYSIS) {
                    echo "LLM Grade: ${env.LLM_GRADE}"
                }
            }
        }
        
        success {
            echo "‚úÖ Test completed successfully!"
            echo "üìä Results: ${env.TOTAL_REQUESTS} requests, ${env.SUCCESS_RATE}% success rate"
            echo "‚è±Ô∏è Avg Response Time: ${env.AVG_RESPONSE_TIME}ms"
            echo "üìà Requests/sec: ${env.REQUESTS_PER_SEC}"
            echo "üìã Check the HTML report for detailed results"
        }
        
        failure {
            echo "‚ùå Test execution failed!"
            echo "üìã Check the console output for error details"
            echo "üîß Verify your script and parameters"
        }
        
        unstable {
            echo "‚ö†Ô∏è Test completed with warnings"
            echo "üìä Performance thresholds not met"
            echo "üîç Issues: ${env.PERFORMANCE_ISSUES}"
        }
    }
} 