pipeline {
    agent any
    
    parameters {
        // Test execution parameters
        string(name: 'SELECTED_SCRIPT', defaultValue: '', description: 'Script to run (will be populated from available scripts)')
        string(name: 'TARGET_HOST', defaultValue: 'https://api.example.com', description: 'Target host URL for testing')
        string(name: 'API_TOKEN', defaultValue: '', description: 'API token for authentication (optional)')
        choice(name: 'ENVIRONMENT', choices: ['dev', 'staging', 'prod'], description: 'Target environment')
        
        // Test configuration
        string(name: 'USERS', defaultValue: '10', description: 'Number of concurrent users')
        string(name: 'SPAWN_RATE', defaultValue: '2', description: 'User spawn rate per second')
        string(name: 'RUN_TIME', defaultValue: '5m', description: 'Test duration (e.g., 5m, 10m, 1h)')
        string(name: 'MIN_WAIT', defaultValue: '1000', description: 'Minimum wait time between requests (ms)')
        string(name: 'MAX_WAIT', defaultValue: '5000', description: 'Maximum wait time between requests (ms)')
        
        // Advanced options
        booleanParam(name: 'USE_LLM_ANALYSIS', defaultValue: false, description: 'Enable LLM-powered analysis')
        booleanParam(name: 'GENERATE_HTML_REPORT', defaultValue: true, description: 'Generate HTML test report')
        booleanParam(name: 'GENERATE_CSV_REPORT', defaultValue: true, description: 'Generate CSV test report')
        string(name: 'LOG_LEVEL', defaultValue: 'INFO', description: 'Logging level (DEBUG, INFO, WARNING, ERROR)')
        
        // Performance thresholds
        string(name: 'MAX_AVG_RESPONSE_TIME', defaultValue: '2000', description: 'Maximum average response time (ms)')
        string(name: 'MIN_SUCCESS_RATE', defaultValue: '95', description: 'Minimum success rate percentage')
        string(name: 'MIN_REQUESTS_PER_SEC', defaultValue: '10', description: 'Minimum requests per second')
        
        // Test metadata
        string(name: 'TEST_DESCRIPTION', defaultValue: '', description: 'Description of this test run (optional)')
        string(name: 'TEST_TAGS', defaultValue: '', description: 'Tags for this test run (optional)')
    }
    
    environment {
        // Python and workspace configuration
        PYTHON_VERSION = "${env.PYTHON_VERSION ?: '3.9'}"
        WORKSPACE_DIR = "${WORKSPACE}\\test_workspace"
        REPORTS_DIR = "${WORKSPACE}\\test_reports"
        SCRIPTS_DIR = "${WORKSPACE}\\generated_scripts"
        TIMESTAMP = "${new Date().format('yyyyMMdd_HHmmss')}"
        
        // Git configuration
        GIT_SCRIPTS_BRANCH = "${env.GIT_SCRIPTS_BRANCH ?: 'generated-scripts'}"
        
        // Build information
        BUILD_INFO = "Build #${env.BUILD_NUMBER} - ${env.BUILD_ID}"
        JOB_NAME = "${env.JOB_NAME}"
        BUILD_URL = "${env.BUILD_URL}"
        
        // Test execution settings
        TEST_RUN_ID = "${env.BUILD_NUMBER}_${TIMESTAMP}"
        TEST_ENVIRONMENT = "${params.ENVIRONMENT}"
        
        // Performance thresholds (convert to integers)
        MAX_RESPONSE_TIME_MS = "${params.MAX_AVG_RESPONSE_TIME}"
        MIN_SUCCESS_RATE_PCT = "${params.MIN_SUCCESS_RATE}"
        MIN_REQUESTS_PER_SEC = "${params.MIN_REQUESTS_PER_SEC}"
    }
    
    stages {
        stage('Checkout') {
            steps {
                checkout scm
                script {
                    echo "=== Test Execution Pipeline ==="
                    echo "Build Number: ${env.BUILD_NUMBER}"
                    echo "Job Name: ${env.JOB_NAME}"
                    echo "Workspace: ${WORKSPACE}"
                    echo "Branch: ${env.BRANCH_NAME}"
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    echo "Environment: ${TEST_ENVIRONMENT}"
                    echo "Target Host: ${params.TARGET_HOST}"
                }
            }
        }
        
        stage('Setup Environment') {
            steps {
                script {
                    echo "Setting up test environment..."
                    echo "Python Version: ${PYTHON_VERSION}"
                    echo "Scripts Branch: ${GIT_SCRIPTS_BRANCH}"
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    echo "Performance Thresholds:"
                    echo "  Max Response Time: ${MAX_RESPONSE_TIME_MS}ms"
                    echo "  Min Success Rate: ${MIN_SUCCESS_RATE_PCT}%"
                    echo "  Min Requests/sec: ${MIN_REQUESTS_PER_SEC}"
                    
                    // Create directories
                    bat "if not exist \"${WORKSPACE_DIR}\" mkdir \"${WORKSPACE_DIR}\""
                    bat "if not exist \"${REPORTS_DIR}\" mkdir \"${REPORTS_DIR}\""
                    bat "if not exist \"${SCRIPTS_DIR}\" mkdir \"${SCRIPTS_DIR}\""
                    
                    // Setup Python virtual environment
                    bat """
                        if not exist venv (
                            python -m venv venv
                        )
                        call venv\\Scripts\\activate.bat
                        pip install --upgrade pip
                        pip install -r requirements.txt
                        pip install -e .
                    """
                }
            }
        }
        
        stage('List Available Scripts') {
            steps {
                script {
                    echo "Fetching available scripts from Git..."
                    echo "Branch: ${GIT_SCRIPTS_BRANCH}"
                    
                    // Fetch scripts branch
                    bat """
                        git fetch origin ${GIT_SCRIPTS_BRANCH}:${GIT_SCRIPTS_BRANCH} || echo "Branch not found"
                        git checkout ${GIT_SCRIPTS_BRANCH} || echo "Could not checkout scripts branch"
                    """
                    
                    // List available scripts
                    def scriptFiles = bat(script: "dir ${SCRIPTS_DIR}\\*.py /b", returnStdout: true).trim()
                    if (scriptFiles) {
                        echo "Available scripts:"
                        scriptFiles.split('\n').each { script ->
                            echo "  - ${script}"
                        }
                        
                        // Store available scripts in environment
                        env.AVAILABLE_SCRIPTS = scriptFiles
                        env.SCRIPTS_COUNT = scriptFiles.split('\n').size()
                        
                        // If no script selected, use the first one
                        if (!params.SELECTED_SCRIPT) {
                            def firstScript = scriptFiles.split('\n')[0]
                            env.SELECTED_SCRIPT = firstScript
                            echo "Auto-selected script: ${firstScript}"
                        } else {
                            env.SELECTED_SCRIPT = params.SELECTED_SCRIPT
                        }
                    } else {
                        error "No scripts found in ${SCRIPTS_DIR}. Please run the Script Generation Pipeline first."
                    }
                    
                    // Return to main branch
                    bat "git checkout main"
                    
                    echo "Selected script: ${env.SELECTED_SCRIPT}"
                    echo "Total scripts available: ${env.SCRIPTS_COUNT}"
                }
            }
        }
        
        stage('Validate Script') {
            steps {
                script {
                    echo "Validating selected script: ${env.SELECTED_SCRIPT}"
                    
                    // Check if script exists
                    def scriptExists = fileExists "${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}"
                    if (!scriptExists) {
                        error "Script ${env.SELECTED_SCRIPT} not found. Please check the script name."
                    }
                    
                    // Store script info in environment
                    env.SCRIPT_NAME = env.SELECTED_SCRIPT.replace('.py', '')
                    env.SCRIPT_PATH = "${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}"
                    
                    echo "Script validation successful"
                    echo "Script Name: ${env.SCRIPT_NAME}"
                    echo "Script Path: ${env.SCRIPT_PATH}"
                }
            }
        }
        
        stage('Run Test') {
            steps {
                script {
                    echo "Running test with script: ${env.SELECTED_SCRIPT}"
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    echo "Environment: ${TEST_ENVIRONMENT}"
                    echo "Target Host: ${params.TARGET_HOST}"
                    echo "Users: ${params.USERS}"
                    echo "Run Time: ${params.RUN_TIME}"
                    
                    // Create test configuration
                    def testConfig = [
                        scenario_name: env.SCRIPT_NAME,
                        host: params.TARGET_HOST,
                        users: params.USERS,
                        spawn_rate: params.SPAWN_RATE,
                        run_time: params.RUN_TIME,
                        min_wait: params.MIN_WAIT,
                        max_wait: params.MAX_WAIT,
                        assertions: [
                            [type: "status_code", value: 200]
                        ],
                        extract_variables: [:],
                        headers: [
                            "Content-Type": "application/json",
                            "User-Agent": "Jenkins-Locust-AI-Agent/1.0"
                        ],
                        params: [:],
                        body: [:],
                        output_dir: REPORTS_DIR,
                        generate_csv: params.GENERATE_CSV_REPORT,
                        generate_html: params.GENERATE_HTML_REPORT,
                        log_level: params.LOG_LEVEL
                    ]
                    
                    writeFile file: "${WORKSPACE_DIR}\\test_config.json", text: groovy.json.JsonOutput.toJson(testConfig)
                    
                    // Fetch the selected script
                    bat """
                        git fetch origin ${GIT_SCRIPTS_BRANCH}:${GIT_SCRIPTS_BRANCH}
                        git checkout ${GIT_SCRIPTS_BRANCH} -- ${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}
                        git checkout main
                    """
                    
                    // Run the test
                    bat """
                        call venv\\Scripts\\activate.bat
                        cd ${WORKSPACE}
                        set PYTHONPATH=%PYTHONPATH%;${WORKSPACE}
                        
                        python -c "
import sys
sys.path.insert(0, '.')
from Locust_AI_Agent.core.test_agent import LocustTestAgent, TestConfig
import json

# Load test config
with open('${WORKSPACE_DIR}\\test_config.json', 'r') as f:
    test_config_data = json.load(f)

# Create test config
test_config = TestConfig(**test_config_data)

# Run test
agent = LocustTestAgent(workspace_dir='${WORKSPACE_DIR}')
script_path = '${SCRIPTS_DIR}\\${env.SELECTED_SCRIPT}'
result = agent.execute_test(script_path, test_config)

# Save results
workflow_result = {
    'workflow_success': result.success,
    'scenario_name': result.scenario_name,
    'script_path': result.script_path,
    'html_report_path': result.html_report_path,
    'csv_report_path': result.csv_report_path,
    'test_metadata': {
        'test_run_id': '${TEST_RUN_ID}',
        'environment': '${TEST_ENVIRONMENT}',
        'build_number': '${env.BUILD_NUMBER}',
        'job_name': '${env.JOB_NAME}',
        'description': '${params.TEST_DESCRIPTION}',
        'tags': '${params.TEST_TAGS}'
    },
    'test_result': {
        'execution_time': result.execution_time,
        'total_requests': result.total_requests,
        'failed_requests': result.failed_requests,
        'avg_response_time': result.avg_response_time,
        'requests_per_sec': result.requests_per_sec
    }
}

with open('${WORKSPACE_DIR}\\test_results.json', 'w') as f:
    json.dump(workflow_result, f, indent=2)

print(f'Test completed: Success={result.success}')
"
                    """
                    
                    // Store test results in environment
                    def resultsText = readFile file: "${WORKSPACE_DIR}\\test_results.json"
                    def results = new groovy.json.JsonSlurper().parseText(resultsText)
                    
                    // Convert to serializable format immediately
                    def serializableResults = [
                        workflow_success: results.workflow_success?.toString(),
                        scenario_name: results.scenario_name?.toString(),
                        script_path: results.script_path?.toString(),
                        test_result: results.test_result ? [
                            execution_time: results.test_result.execution_time?.toString(),
                            total_requests: results.test_result.total_requests?.toString(),
                            failed_requests: results.test_result.failed_requests?.toString(),
                            avg_response_time: results.test_result.avg_response_time?.toString(),
                            requests_per_sec: results.test_result.requests_per_sec?.toString(),
                            html_report_path: results.test_result.html_report_path?.toString(),
                            csv_report_path: results.test_result.csv_report_path?.toString()
                        ] : [:],
                        llm_analysis: results.llm_analysis ? [
                            performance_grade: results.llm_analysis.performance_grade?.toString(),
                            summary: results.llm_analysis.summary?.toString(),
                            key_insights: results.llm_analysis.key_insights?.collect { it.toString() } ?: [],
                            recommendations: results.llm_analysis.recommendations?.collect { it.toString() } ?: []
                        ] : [:]
                    ]
                    
                    env.TEST_SUCCESS = serializableResults.workflow_success
                    env.TOTAL_REQUESTS = serializableResults.test_result.total_requests
                    env.FAILED_REQUESTS = serializableResults.test_result.failed_requests
                    env.AVG_RESPONSE_TIME = serializableResults.test_result.avg_response_time
                    env.REQUESTS_PER_SEC = serializableResults.test_result.requests_per_sec
                    env.EXECUTION_TIME = serializableResults.test_result.execution_time
                    
                    // Store serializable results for later use
                    env.SERIALIZABLE_TEST_RESULTS = groovy.json.JsonOutput.toJson(serializableResults)
                    
                    echo "Test execution completed"
                    echo "Success: ${env.TEST_SUCCESS}"
                    echo "Total Requests: ${env.TOTAL_REQUESTS}"
                    echo "Failed Requests: ${env.FAILED_REQUESTS}"
                    echo "Avg Response Time: ${env.AVG_RESPONSE_TIME}ms"
                    echo "Requests/sec: ${env.REQUESTS_PER_SEC}"
                }
            }
        }
        
        stage('LLM Analysis') {
            when {
                expression { params.USE_LLM_ANALYSIS }
            }
            steps {
                script {
                    echo "Running LLM analysis..."
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    
                    withCredentials([string(credentialsId: 'openai-api-key', variable: 'OPENAI_API_KEY')]) {
                        bat """
                            call venv\\Scripts\\activate.bat
                            cd ${WORKSPACE}
                            set PYTHONPATH=%PYTHONPATH%;${WORKSPACE}
                            set OPENAI_API_KEY=${OPENAI_API_KEY}
                            
                            python -c "
import sys
sys.path.insert(0, '.')
from Locust_AI_Agent.analysis.llm_analyzer import LLMAnalyzer
import json

# Load test results
with open('${WORKSPACE_DIR}\\test_results.json', 'r') as f:
    results = json.load(f)

# Run LLM analysis
llm_analyzer = LLMAnalyzer(api_key='${OPENAI_API_KEY}')
llm_analysis = llm_analyzer.analyze_test_results(
    results['test_result'],
    results.get('html_report_path')
)

# Add LLM analysis to results
results['llm_analysis'] = llm_analysis

# Save updated results
with open('${WORKSPACE_DIR}\\test_results_with_llm.json', 'w') as f:
    json.dump(results, f, indent=2)

print('LLM analysis completed')
"
                        """
                    }
                    
                    echo "LLM analysis completed successfully"
                }
            }
        }
        
        stage('Archive Results') {
            steps {
                script {
                    echo "Archiving test results..."
                    echo "Test Run ID: ${TEST_RUN_ID}"
                    
                    // Archive test results and reports
                    archiveArtifacts artifacts: "test_workspace\\test_results.json", fingerprint: true
                    
                    if (params.USE_LLM_ANALYSIS) {
                        archiveArtifacts artifacts: "test_workspace\\test_results_with_llm.json", fingerprint: true
                    }
                    
                    // Archive generated reports
                    if (params.GENERATE_HTML_REPORT) {
                        archiveArtifacts artifacts: "test_reports\\**\\*.html", fingerprint: true
                    }
                    
                    if (params.GENERATE_CSV_REPORT) {
                        archiveArtifacts artifacts: "test_reports\\**\\*.csv", fingerprint: true
                    }
                    
                    echo "Results archived successfully"
                }
            }
        }
        
        stage('Analyze Results') {
            steps {
                script {
                    echo "Analyzing test results..."
                    echo "Performance Thresholds:"
                    echo "  Max Response Time: ${MAX_RESPONSE_TIME_MS}ms"
                    echo "  Min Success Rate: ${MIN_SUCCESS_RATE_PCT}%"
                    echo "  Min Requests/sec: ${MIN_REQUESTS_PER_SEC}"
                    
                    // Load and analyze test results
                    def resultsText = readFile file: "test_workspace\\test_results.json"
                    def results = new groovy.json.JsonSlurper().parseText(resultsText)
                    
                    // Convert to serializable format immediately
                    def serializableResults = [
                        workflow_success: results.workflow_success?.toString(),
                        scenario_name: results.scenario_name?.toString(),
                        script_path: results.script_path?.toString(),
                        test_result: results.test_result ? [
                            execution_time: results.test_result.execution_time?.toString(),
                            total_requests: results.test_result.total_requests?.toString(),
                            failed_requests: results.test_result.failed_requests?.toString(),
                            avg_response_time: results.test_result.avg_response_time?.toString(),
                            requests_per_sec: results.test_result.requests_per_sec?.toString(),
                            html_report_path: results.test_result.html_report_path?.toString(),
                            csv_report_path: results.test_result.csv_report_path?.toString()
                        ] : [:],
                        llm_analysis: results.llm_analysis ? [
                            performance_grade: results.llm_analysis.performance_grade?.toString(),
                            summary: results.llm_analysis.summary?.toString(),
                            key_insights: results.llm_analysis.key_insights?.collect { it.toString() } ?: [],
                            recommendations: results.llm_analysis.recommendations?.collect { it.toString() } ?: []
                        ] : [:]
                    ]
                    
                    echo "Test Results Analysis:"
                    echo "  Success: ${serializableResults.workflow_success}"
                    echo "  Scenario: ${serializableResults.scenario_name}"
                    echo "  Script: ${env.SELECTED_SCRIPT}"
                    echo "  Test Run ID: ${TEST_RUN_ID}"
                    
                    if (serializableResults.test_result) {
                        def testResult = serializableResults.test_result
                        echo "  Execution Time: ${testResult.execution_time}s"
                        echo "  Total Requests: ${testResult.total_requests}"
                        echo "  Failed Requests: ${testResult.failed_requests}"
                        echo "  Avg Response Time: ${testResult.avg_response_time}ms"
                        echo "  Requests/sec: ${testResult.requests_per_sec}"
                        
                        // Calculate success rate
                        def successRate = testResult.total_requests.toInteger() > 0 ? 
                            ((testResult.total_requests.toInteger() - testResult.failed_requests.toInteger()) / testResult.total_requests.toInteger() * 100) : 0
                        echo "  Success Rate: ${successRate}%"
                        
                        // Store calculated values in environment
                        env.SUCCESS_RATE = successRate
                        
                        // Performance threshold validation
                        def performanceIssues = []
                        
                        if (testResult.avg_response_time.toInteger() > MAX_RESPONSE_TIME_MS.toInteger()) {
                            performanceIssues.add("Average response time (${testResult.avg_response_time}ms) exceeds threshold (${MAX_RESPONSE_TIME_MS}ms)")
                        }
                        
                        if (successRate < MIN_SUCCESS_RATE_PCT.toInteger()) {
                            performanceIssues.add("Success rate (${successRate}%) below threshold (${MIN_SUCCESS_RATE_PCT}%)")
                        }
                        
                        if (testResult.requests_per_sec.toDouble() < MIN_REQUESTS_PER_SEC.toInteger()) {
                            performanceIssues.add("Requests per second (${testResult.requests_per_sec}) below threshold (${MIN_REQUESTS_PER_SEC})")
                        }
                        
                        if (performanceIssues.size() > 0) {
                            echo "Performance Issues Detected:"
                            performanceIssues.each { issue ->
                                echo "  - ${issue}"
                            }
                            currentBuild.result = 'UNSTABLE'
                            env.PERFORMANCE_ISSUES = performanceIssues.join('; ')
                        } else {
                            echo "All performance thresholds met!"
                            env.PERFORMANCE_ISSUES = "None"
                        }
                    }
                    
                    // LLM analysis summary
                    if (params.USE_LLM_ANALYSIS && serializableResults.llm_analysis) {
                        def llmAnalysis = serializableResults.llm_analysis
                        echo "LLM Analysis:"
                        echo "  Performance Grade: ${llmAnalysis.performance_grade}"
                        echo "  Summary: ${llmAnalysis.summary}"
                        
                        env.LLM_GRADE = llmAnalysis.performance_grade
                        env.LLM_SUMMARY = llmAnalysis.summary
                        
                        if (llmAnalysis.key_insights) {
                            echo "  Key Insights:"
                            llmAnalysis.key_insights.each { insight ->
                                echo "    • ${insight}"
                            }
                        }
                        
                        if (llmAnalysis.recommendations) {
                            echo "  Recommendations:"
                            llmAnalysis.recommendations.each { rec ->
                                echo "    • ${rec}"
                            }
                        }
                    }
                }
            }
        }
        
        stage('Publish HTML Report') {
            when {
                expression { params.GENERATE_HTML_REPORT }
            }
            steps {
                script {
                    echo "Publishing HTML report..."
                    
                    // Check if HTML report exists
                    def htmlReportExists = fileExists "test_reports\\*.html"
                    if (htmlReportExists) {
                        publishHTML([
                            allowMissing: false,
                            alwaysLinkToLastBuild: true,
                            keepAll: true,
                            reportDir: 'test_reports',
                            reportFiles: '*.html',
                            reportName: 'Locust Test Report',
                            reportTitles: 'Performance Test Results'
                        ])
                        echo "HTML report published successfully"
                    } else {
                        echo "No HTML report found to publish"
                    }
                }
            }
        }
    }
    
    post {
        always {
            script {
                echo "=== Test Execution Pipeline Summary ==="
                echo "Build Number: ${env.BUILD_NUMBER}"
                echo "Job Name: ${env.JOB_NAME}"
                echo "Result: ${currentBuild.result}"
                echo "Build URL: ${env.BUILD_URL}"
                echo "Test Run ID: ${TEST_RUN_ID}"
                echo "Script Used: ${env.SELECTED_SCRIPT}"
                echo "Environment: ${TEST_ENVIRONMENT}"
                echo "Target Host: ${params.TARGET_HOST}"
                echo "Test Success: ${env.TEST_SUCCESS}"
                echo "Total Requests: ${env.TOTAL_REQUESTS}"
                echo "Success Rate: ${env.SUCCESS_RATE}%"
                echo "Avg Response Time: ${env.AVG_RESPONSE_TIME}ms"
                echo "Performance Issues: ${env.PERFORMANCE_ISSUES}"
                
                if (params.USE_LLM_ANALYSIS) {
                    echo "LLM Grade: ${env.LLM_GRADE}"
                }
            }
        }
        
        success {
            echo "✅ Test completed successfully!"
            echo "📊 Results: ${env.TOTAL_REQUESTS} requests, ${env.SUCCESS_RATE}% success rate"
            echo "⏱️ Avg Response Time: ${env.AVG_RESPONSE_TIME}ms"
            echo "📈 Requests/sec: ${env.REQUESTS_PER_SEC}"
            echo "📋 Check the HTML report for detailed results"
        }
        
        failure {
            echo "❌ Test execution failed!"
            echo "📋 Check the console output for error details"
            echo "🔧 Verify your script and parameters"
        }
        
        unstable {
            echo "⚠️ Test completed with warnings"
            echo "📊 Performance thresholds not met"
            echo "🔍 Issues: ${env.PERFORMANCE_ISSUES}"
        }
    }
} 